External Metrics Ingestion Agent (EMA) is open-source software that you build yourself, to receive metrics data from your software, and export it to Azureâ€™s External Metrics Ingestion Gateway (EMIG). EMA is based on the OpenTelemetry Collector, and the OpenTelemetry Collector is an open-source GitHub project, that has several optional packages to be used as receivers, processors, exporters and extensions. In this document we will talk about the several types of configurations that are required for different topologies between your workloads and the EMIG endpoint(s).


The configuration file of EMA can be divided into four parts; that is receiver, processor, exporter and pipeline, each section defining one or more objects. You can think of these objects as the building blocks of the EMA configuration. Except the pipeline objects, all other objects are based on an OpenTelemetry Collector Golang package. These packages can be either defined in GitHub libraries or custom built by you. There are two such libraries that define several receiver/processor/exporter packages. One of them is the Otel-collector core library that defines the core packages and the other one is contribution library that has user-defined packages. Also, if necessary, even new packages can be added to the contribution library. The links of both the core and contribution libraries are listed below for each type of packages. For further details on a specific package configuration, please refer to the documentation provided in the specific GitHub package.


Next we will discuss the EMA configuration for receiver, exporter, processor and pipeline sections one by one.

1. Each receiver object describes the Otel-collector package to be used for receiving the metrics. Each package uses a specific protocol to receive the metrics. Under the package, you need to put the package specific configuration, such as network endpoint (where the metrices are received).
For example, please refer to the configuration Multiple_Workload_Multiple_Receiver_Config.yaml file receiver section. Here we have shown two receivers, both using the otlpreceiver package. otlp/1 and otlp/2 are the receiver objects. OTLP protocol is using the GRPC protocol for underlaying communication here. The receiver objects, otlp/1 and otlp/2 are differentiated by their endpoint ports. This type of configuration is used if you want to receive two parallel streams of matrices from two separate workloads.

    Otel-collector receiver package list: https://github.com/open-telemetry/opentelemetry-collector/tree/main/receiver, https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver

2. Processor objects define a processor package and the configuration specific to it. For example, you can refer to the Multiple_Workload_Single_Receiver_Config.yaml file processor section. Here we have used two processor packages, batchprocessor and filterprocessor. The batchprocessor serializes the metrics received from multiple workloads. The filterprocessor filters out metrics received from a single workload among many and only propagates metrics from that workload, dropping metrics from all others. This processor is required, if for some reason, you cannot use separate receiver endpoints in your EMA to receive metrics from separate workloads, but still need to publish the metrics to separate EMIG endpoints based on a particular dimension in your metrics. This dimension can be added by each of your workloads to distinguish the metrics from each other. Here, we have taken the dimension 'site' as the separator and two unique values for it as, 'London' and 'Kolkata'.

    Otel-collector processor package list: https://github.com/open-telemetry/opentelemetry-collector/tree/main/processor, https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor

3. Exporter objects define the package to be used to export the metrics after processing is done. EMA will only export to the EMIG endpoints and EMIG accepts only Kafka protocol. So, for all your purposes, you need to use the kafkaexporter package here.
For example, please refer to the Multiple_Workload_Multiple_Receiver_Config.yaml file. Here, you have two exporters, both using the kafkaexporter package. kafka/1 and kafka/2 are the exporter objects. The password in each of the objects is read from environment variables. The topic and password differentiate these objects, each exporting metrics to a separate EMIG endpoint.

    Otel-collector processor package list: https://github.com/open-telemetry/opentelemetry-collector/tree/main/exporter, https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/exporter

4. The last section of the EMA configuration is the pipelines. In this section, the objects that you have defined till now, need to be arranged in such a way that metrics received from a receiver object get processed by a sequence of processor objects and ultimately exported by an exporter object. Thus, you can see, the pipeline object itself has three sections inside, listing the receiver, processor, and exporter objects. You can define as many pipelines as you need. For various types of topologies, EMA will need four types of pipelines, that we are going to discuss now one by one.

    	a. If you have a single workload and want to export the metrics to a single EMIG endpoint, the pipeline is very straight forward. Define one receiver (with protocol of your choice that can talk to you workload), one batch processor and one kafka exporter (having the details of the EMIG endpoint). Then create a pipeline with all these three objects. Example: Single_Workload_Config.yaml.
        b. If you have multiple workloads and you need to export the metrics to separate EMIG endpoints for each of them, you can achieve this in two ways. The simplest approach is to run one EMA per workload and then use a separate Single_Workload_Config.yaml for each one of them.
    	c. In the above case, one more approach is to run a single EMA that can receive metrics from each of your workloads through a separate receiver object, either separated by port, IP or a combination of both. In this case, to entertain X workloads, you will require X receivers, X kafkaexporters and a single batch processor. Then at the end you need to arrange these objects into X separate pipelines. Here you will need an exclusive one-to-one mapping of each of the receiver, exporter and pipelines. Example: Multiple_Workload_Multiple_Receiver_Config.yaml.
        d. The most complicated case is where you have multiple workloads and EMIG endpoints but due to some design constraints, you are not able to use multiple EMAs per workloads or multiple receiver endpoints(either because there is a metrics aggregator between your workloads and the EMA, or there are limitations on number of ports/IPs in your EMA system). In this case you need the filter processor to filter out metrics from a particular workload and propagate it to the respective EMIG endpoint. Here, you will receive all the metrics from X workloads to a single receiver, but you will have X filterprocessor objects defined to accept metrics from only a single workload(using matching logic on a dimension specifically added with unique values by each of the workloads). You also need X exporters talking to X EMIG endpoints, where each pipeline will have an exclusive one-to-one mapping among the workload, filterprocessor object and kafkaexporter object. Example: Multiple_Workload_Single_Receiver_Config.yaml.

All the example configurations are uploaded to the same path in GitHub repository.
